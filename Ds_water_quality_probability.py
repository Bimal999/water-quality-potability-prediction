# -*- coding: utf-8 -*-
"""Water Quality Probability.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17aSt2hGLIobMNrOBCeuK-oCrTTaKnm-o
"""

## import packages to be used
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# To scale the data using z-score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Metrics to evaluate the model
from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# For tuning the model
from sklearn.model_selection import GridSearchCV

# To ignore warnings
import warnings
warnings.filterwarnings("ignore")

# Reading the dataset
df = pd.read_csv('/content/water_potability.csv')

# check few observations
df.head()

# Check the variable types
df.info()

# Checking the number of unique values in each column
df.nunique()

## Check percent of missing
(df.isnull().sum() / df.shape[0])*100

num_cols =df.columns
num_cols

df.describe().T

# Creating histograms
df.hist(figsize = (14, 14))
plt.show()

# The mean of numerical variables grouped by attrition
df.groupby(['Potability'])[num_cols].mean()

# The median of numerical variables grouped by attrition
df.groupby(['Potability'])[num_cols].median()

# Plotting the correlation between numerical variables
plt.figure(figsize = (15, 8))
sns.heatmap(df[num_cols].corr(), annot = True, fmt = '0.2f', cmap = 'YlGnBu');

# Input the missing values with the median of their respective columns
df['ph'].fillna(value=df['ph'].median(),inplace=True)
df['Sulfate'].fillna(value=df['Sulfate'].median(),inplace=True)
df['Trihalomethanes'].fillna(value=df['Trihalomethanes'].median(),inplace=True)

# Check the variable types again and wheter the inputation worked
df.info()

"""#EDA Conclusion: After EDA and addressing the missing data, we see that distributions of the different variables have not been impacted drastically. Now we can proceed with modeling."""

# Separating the target variable and other variables
Y = df.Potability
X = df.drop(columns = ['Potability'])

# Scaling the data
sc = StandardScaler()
X_scaled = sc.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns = X.columns)

# Splitting the data
## Splitting the data into 70% train and 30% test sets
X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(X_scaled, Y, test_size = 0.3, random_state = 1, stratify = Y)

## 1. Logistic Regression
from sklearn.linear_model import LogisticRegression
lr_model = LogisticRegression(max_iter=1000, random_state=42)
lr_model.fit(X_train_scaled, y_train)

## 2. Random Forest
from sklearn.ensemble import RandomForestClassifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_scaled, y_train)

## 3. Gradient Boosting
from sklearn.ensemble import GradientBoostingClassifier
gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)
gb_model.fit(X_train_scaled, y_train)

## 4. Support Vector Machine
from sklearn.svm import SVC
svm_model = SVC(random_state=42)
svm_model.fit(X_train_scaled, y_train)

## 5. K-Nearest Neighbors
from sklearn.neighbors import KNeighborsClassifier
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train_scaled, y_train)

## 6. Linear Discriminant Analysis
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
lda_model = LinearDiscriminantAnalysis()
lda_model.fit(X_train_scaled, y_train)

## 7. Quadratic Discriminant Analysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
qda_model = QuadraticDiscriminantAnalysis()
qda_model.fit(X_train_scaled, y_train)

# Placeholder for model predictions on the test set
# Replace `y_pred_model` with your actual predictions
y_pred_lr = lr_model.predict(X_test_scaled)
y_pred_rf = rf_model.predict(X_test_scaled)
y_pred_gb = gb_model.predict(X_test_scaled)
y_pred_svm = svm_model.predict(X_test_scaled)
y_pred_knn = knn_model.predict(X_test_scaled)
y_pred_lda = lda_model.predict(X_test_scaled)
y_pred_qda = qda_model.predict(X_test_scaled)

# Dictionary to hold model names and their corresponding predictions
predictions = {
    'Logistic Regression': y_pred_lr,
    'Random Forest': y_pred_rf,
    'Gradient Boosting': y_pred_gb,
    'Support Vector Machine': y_pred_svm,
    'K-Nearest Neighbors': y_pred_knn,
    'Linear Discriminant Analysis': y_pred_lda,
    'Quadratic Discriminant Analysis': y_pred_qda
}

# Initialize an empty dictionary to store performance metrics
performance_metrics = {}

# Calculate performance metrics for each model and store them in the dictionary
for model_name, y_pred in predictions.items():
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    performance_metrics[model_name] = [accuracy, precision, recall, f1]

# Create a DataFrame from the performance metrics dictionary
performance_df = pd.DataFrame(performance_metrics, index=['Accuracy', 'Precision', 'Recall', 'F1 Score']).T

# Display the summary table
print(performance_df)

"""**Now we will perform some hyperparameter tuning.**     
***The codes have been commented out as they take long to run.***
"""

# Placeholder for model predictions on the test set
# Replace `y_pred_model` with your actual predictions
y_pred_lr2 = lr2_model.predict(X_test_scaled)
y_pred_rf2 = rf2_model.predict(X_test_scaled)
y_pred_gb2 = gb2_model.predict(X_test_scaled)
y_pred_svm2 = svm2_model.predict(X_test_scaled)
y_pred_knn2 = knn2_model.predict(X_test_scaled)
y_pred_lda2 = lda2_model.predict(X_test_scaled)
y_pred_qda2 = qda2_model.predict(X_test_scaled)

# Dictionary to hold model names and their corresponding predictions
predictions = {
    'Logistic Regression': y_pred_lr2,
    'Random Forest': y_pred_rf2,
    'Gradient Boosting': y_pred_gb2,
    'Support Vector Machine': y_pred_svm2,
    'K-Nearest Neighbors': y_pred_knn2,
    'Linear Discriminant Analysis': y_pred_lda2,
    'Quadratic Discriminant Analysis': y_pred_qda2
}

# Initialize an empty dictionary to store performance metrics
performance_metrics = {}

# Calculate performance metrics for each model and store them in the dictionary
for model_name, y_pred in predictions.items():
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    performance_metrics[model_name] = [accuracy, precision, recall, f1]

# Create a DataFrame from the performance metrics dictionary
performance_df = pd.DataFrame(performance_metrics, index=['Accuracy', 'Precision', 'Recall', 'F1 Score']).T

# Display the summary table
print(performance_df)

"""**CONCLUSION**    
-  In conclusion, we see that Hyperparameter Tuning have not resulted into a lot of improvement in the models.
-  In general, not a single model stood out to be the best.
-  Given the poor performances of the models, the data does not seem to be real.

"""

